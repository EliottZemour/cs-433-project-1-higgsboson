####### Methods for ML_project_1 #########
'''
blablabla


'''
import numpy as np
import matplotlib.pyplot as plt
from proj1_helpers import *

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
''' Data Manipulation after preprocessing. Split to obtain train and test dataset
and expand dataset with polynomial expansion '''

def split_data(x, y, ratio, seed=1):
    """
    split the dataset based on the split ratio.
    ratio = 0.8 ==> 80% of data set dedicated to training
    """

    data_size = round(ratio * x.shape[0])
    np.random.seed(seed)

    train_indices = np.random.permutation(np.arange(x.shape[0]))[0:data_size]
    train_x = x[train_indices]
    test_x = np.delete(x, train_indices, axis=0)
    train_y = y[train_indices]
    test_y = np.delete(y, train_indices, axis=0)

    return train_x, train_y, test_x, test_y

def buildpoly(tx,degree) :
    '''Compute the extended matrix of features definded by polynomial 
    expansion. For matrix tx as input, the result has the shape 
    result=[tx, tx^2, tx^3,...,tx^(degree)] with tx^j means that all components
    of the tx matrix are raised at power j. '''
    result= tx
    for i in range(1,degree):
        power=tx[:,1:tx.shape[1]]**(i+1)
        result=np.concatenate((result,power),axis=1)
    return result
        

# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

''' Definition of the loss function, its grandient with and without Ridge 
regularization'''


def calculate_hessian(y, tx, w,lambda_):
    """return the Hessian of the loss function."""
    # ***************************************************
    t = (sigmoid(tx @ w) * (1 - sigmoid(tx @ w)))
    sx = np.zeros(tx.shape)
    lambdas = 2*lambda_*np.eye(len(w))

    for i in range(len(t)):
        sx[i] = t[i]*tx[i]
    Hess = (tx.T)@sx + lambdas
    return Hess#/(len(tx))

def sigmoid(t):
    """Apply the sigmoid function on t. """
    return 1/(1+np.exp(-t))

def logsig(x):
    '''Compute the log-sigmoid function component-wise based on MÃ¤chler, Martin 
    paper which prevent NaN issue for instance generated by ln(exp(800)) for instance.'''
    logsig = np.zeros_like(x)
    index0 = x < -33
    logsig[index0] = x[index0]
    index1 = (x >= -33) & (x < -18)
    logsig[index1] = x[index1] - np.exp(x[index1])
    index2 = (x >= -18) & (x < 37)
    logsig[index2] = -np.log1p(np.exp(-x[index2]))
    index3 = x >= 37
    logsig[index3] = -np.exp(-x[index3])
    return logsig


def compute_loss(y, tx, w):
    '''Compute the loss function for a logistic model'''
    z = np.dot(tx, w)
    y = np.asarray(y)
    return np.mean((1 - y) * z - logsig(z))


def compute_sigy(x, y):
    ''' Compute sig(x)-y composent-wise
    with sig(x)=1/(1+exp(-x))'''
    index = x < 0
    result = np.zeros_like(x)
    exp_x = np.exp(x[index])
    y_index = y[index]
    result[index] = ((1 - y_index) * exp_x - y_index) / (1 + exp_x)
    exp_nx = np.exp(-x[~index])
    y_nidx = y[~index]
    result[~index] = ((1 - y_nidx) - y_nidx * exp_nx) / (1 + exp_nx)
    return result


def compute_grad(y, tx, w):
    '''Computes the gradient of the logistic loss. '''
    z = tx.dot(w)
    s = compute_sigy(z, y)
    return tx.T.dot(s) / tx.shape[0]


def compute_grad_ridge(y, tx, w, lambda_):
    '''Computes the gradient for the regularized logistic loss with lambda_
    parameter for Ridge. '''
    grad = compute_grad(y, tx, w)
    return (grad + 2 * lambda_ * w)


def loglogi(y, tx, w, ridge=False, lambda_=0):  # Retourne la prediction y_hat let la log-likelihood
    # L=-(y*np.log(1+np.exp(-tx@))+(1-y)*np.log(1+np.exp(1+np.exp(tx@w))))
    # si ridge = True, on retourne la log_likelihood + lambda_*norme w
    # L = -(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))
    y1 = (tx @ w)
    yhat = sigmoid(y1)
    L = -y * y1 + np.log1p(np.exp(y1))
    # L=-y*y1+y1
    LL = np.sum(L, axis=0)
    if (ridge):
        LL = LL  # + lambda_ * np.power(np.linalg.norm(w),2)

    return yhat, np.double(LL)


# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

''' Descent algorithms to minimize the loss function, with and without Ridge 
regularization '''


def gd_log_ridge_newton(y, tx, initial_w, max_iters, gamma, lambda_):
    # Define parameters to store w and loss
    losses = []
    w = initial_w
    n_iter = 1
    loss = 10
    loss2 = 15
    while ((n_iter < max_iters)):  # and (np.abs(loss2-loss)>1e-5)):
        loss = compute_loss(y, tx, w)
        grad = compute_grad_ridge(y, tx, w, lambda_)
        Hessian = calculate_hessian(y, tx, w, lambda_)
        gr = np.linalg.inv(Hessian) @ grad
        w2 = w - gamma * gr / np.linalg.norm(gr)

        loss2 = compute_loss(y, tx, w2)
        if (loss2 < loss):
            w = w2
            gamma = 1.3 * gamma
        else:
            gamma = 0.95 * gamma
        # gamma=(np.linalg.norm(gr)/gamma)
        # losses.append(loss)
        print(loss)
        n_iter = n_iter + 1
    return losses, w, Hessian


def gradient_descent_log(y, tx, initial_w, max_iters, gamma):
    '''Gradient descent algorithm.
    INPUT : 
    y : tje true output result
    tx : fetures matrix
    initial_w : initial w vector to begin the descent
    max_iters : The maximum nomber of descent step
    gamma = the descent step length
    OUTPUT 
    loss : The loss function value at the end of the descent
    w : w-vector at the end of the algorithm
        '''
    w = initial_w
    n_iter = 1
    loss = 10
    loss2=15
    while ((n_iter < max_iters) and (np.abs(loss2 - loss) > 1e-8)):
        loss = compute_loss(y, tx, w)
        grad = compute_grad(y, tx, w)

        w = w - gamma * grad

        y_hat2, loss2 = loglogi(y, tx, w)
        n_iter = n_iter + 1

    return loss, w


def gd_log_ridge(y, tx, initial_w, max_iters, gamma, lambda_):
    '''Same as gradient_descent_log() with ridge regularization'''
    w = initial_w
    n_iter = 1
    loss = 10
    loss2 = 15

    while (((n_iter < max_iters)) and (np.abs(loss2 - loss) > 1e-8)):
       
        loss = compute_loss(y, tx, w)
        grad = compute_grad_ridge(y, tx, w, lambda_)

        w = w - gamma * grad
        loss2 = compute_loss(y, tx, w)
        
        n_iter = n_iter + 1
       
    return loss, w
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
''' Cross Validation '''

def build_k_indices(y, k_fold, seed):
    """build k indices for k-fold."""
    num_row = y.shape[0]
    interval = int(num_row / k_fold)
    np.random.seed(seed)
    indices = np.random.permutation(num_row)
    k_indices = [indices[k * interval: (k + 1) * interval]
                 for k in range(k_fold)]
    return np.array(k_indices)    
    


def optimisation(y,tx, initial_w, max_iter, gamma, lambda_,k=5,seed=30):
   
    k_indices=build_k_indices(y,k,seed)
    w=np.zeros((k,tx.shape[1]))   
    losstest=np.zeros(k)
    losstrain=np.zeros(k)
    for i in range(k):
         grouptestx=tx[k_indices[i,:],:]
         grouptesty=y[k_indices[i,:]]

         grouptrainx=np.delete(tx,k_indices[i,:],axis=0)
         grouptrainy=np.delete(y,k_indices[i,:])
         
         losstrain[i], w[i,:]=gd_log_ridge(grouptrainy,grouptrainx,initial_w,max_iter,gamma,lambda_)
         losstest[i]=compute_loss(grouptesty,grouptestx,w[i,:])
         
    return w,losstest,losstrain


#%%
''' Compute Score'''

def learning(ytrain, xtrain, xtest, initial_w, max_iter, gamma, lambda_, threshold, method):
    if method == 'Log' or method == 'log':
        loss, w2 = gd_log_ridge(ytrain, xtrain, initial_w, max_iter, gamma, lambda_)
    else:
        raise Exception('No Method chosen')
    yhat = sigmoid(xtest @ w2)
    ### Testing the results ###

    y_pred = np.empty(yhat.shape)
    y_pred[np.where(yhat <= threshold)] = 0
    y_pred[np.where(yhat > threshold)] = 1
    return w2, loss, yhat, y_pred

def compute_score(ytest, ypred): return float(sum(i1 == i2 for i1, i2 in zip(ytest, ypred))) / float(len(ytest))


def k_nearest(ytrain, xtrain, xtest, threshold, k):
    dist = np.zeros(np.shape(ytrain))
    minimid = np.ones(k)
    minimval = np.ones(k) * np.inf
    n = np.shape(xtest)
    n = n[0]
    n2 = np.shape(ytrain)
    n2 = n2[0]
    yhat = np.zeros(n)
    print(n)
    for i in range(n):
        print(i)
        for j in range(n2):
            dist[j] = np.linalg.norm(xtest[i] - xtrain[j])
            if dist[j] < np.max(minimval):
                indi = np.argmax(minimval)
                minimval[indi] = dist[j]
                minimid[indi] = j
        wnn = np.reciprocal(minimval)
        ynn = ytrain[minimid.astype(int)]
        yhat[i] = 1 / np.sum(wnn) * np.sum(wnn @ ynn)
    y_pred = np.empty(yhat.shape)
    y_pred[np.where(yhat <= threshold)] = 0
    y_pred[np.where(yhat > threshold)] = 1
    return y_pred



